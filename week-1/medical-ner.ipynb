{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10720543,"sourceType":"datasetVersion","datasetId":6645382},{"sourceId":10720678,"sourceType":"datasetVersion","datasetId":6645488}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:29:19.151161Z","iopub.execute_input":"2025-02-11T09:29:19.151545Z","iopub.status.idle":"2025-02-11T09:29:23.520836Z","shell.execute_reply.started":"2025-02-11T09:29:19.151516Z","shell.execute_reply":"2025-02-11T09:29:23.519723Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.28.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"import os\nfrom typing import List, Dict, Tuple\n\nclass Preprocessing_Maccrobat:\n    def __init__(self, dataset_folder, tokenizer):\n        self.file_ids = [f.split(\".\")[0] for f in os.listdir(dataset_folder) if f.endswith('.txt')]\n        self.text_files = [f + \".txt\" for f in self.file_ids]\n        self.anno_files = [f + \".ann\" for f in self.file_ids]\n        self.num_samples = len(self.file_ids)\n        \n        # Load text data\n        self.texts: List[str] = []\n        for file in self.text_files:\n            with open(os.path.join(dataset_folder, file), 'r') as f:\n                self.texts.append(f.read())\n        \n        # Load annotations\n        self.tags: List[Dict[str, str]] = []\n        for file in self.anno_files:\n            with open(os.path.join(dataset_folder, file), 'r') as f:\n                text_bound_ann = [t.split(\"\\t\") for t in f.read().split(\"\\n\") if t.startswith(\"T\")]\n                text_bound_lst = []\n                for text_b in text_bound_ann:\n                    label = text_b[1].split(\" \")\n                    try:\n                        _ = int(label[1])\n                        _ = int(label[2])\n                        tag = {\n                            \"text\": text_b[-1],\n                            \"label\": label[0],\n                            \"start\": label[1],\n                            \"end\": label[2]\n                        }\n                        text_bound_lst.append(tag)\n                    except:\n                        pass\n                self.tags.append(text_bound_lst)\n        \n        self.tokenizer = tokenizer\n\n    def process(self) -> Tuple[List[List[str]], List[List[str]]]:\n        input_texts, input_labels = [], []\n        \n        for idx in range(self.num_samples):\n            full_text = self.texts[idx]\n            tags = self.tags[idx]\n\n            label_offset, continuous_label_offset = [], []\n            for tag in tags:\n                offset = list(range(int(tag[\"start\"]), int(tag[\"end\"]) + 1))\n                label_offset.append(offset)\n                continuous_label_offset.extend(offset)\n            \n            all_offset = list(range(len(full_text)))\n            zero_offset = [offset for offset in all_offset if offset not in continuous_label_offset]\n            zero_offset = Preprocessing_Maccrobat.find_continuous_ranges(zero_offset)\n\n            self.tokens, self.labels = [], []\n            self._merge_offset(full_text, tags, zero_offset, label_offset)\n\n            assert len(self.tokens) == len(self.labels), \"Length of tokens and labels are not equal\"\n\n            input_texts.append(self.tokens)\n            input_labels.append(self.labels)\n\n        return input_texts, input_labels\n\n    def _merge_offset(self, full_text, tags, zero_offset, label_offset):\n        i = j = 0\n        while i < len(zero_offset) and j < len(label_offset):\n            if zero_offset[i][0] < label_offset[j][0]:\n                self._add_zero(full_text, zero_offset, i)\n                i += 1\n            else:\n                self._add_label(full_text, label_offset, j, tags)\n                j += 1\n\n        while i < len(zero_offset):\n            self._add_zero(full_text, zero_offset, i)\n            i += 1\n\n        while j < len(label_offset):\n            self._add_label(full_text, label_offset, j, tags)\n            j += 1\n\n    def _add_zero(self, full_text, offset, index):\n        start, *_, end = offset[index] if len(offset[index]) > 1 else (offset[index][0], offset[index][0] + 1)\n        text = full_text[start:end]\n        text_tokens = self.tokenizer.tokenize(text)\n\n        self.tokens.extend(text_tokens)\n        self.labels.extend([\"O\"] * len(text_tokens))\n\n    def _add_label(self, full_text, offset, index, tags):\n        start, *_, end = offset[index] if len(offset[index]) > 1 else (offset[index][0], offset[index][0] + 1)\n        text = full_text[start:end]\n        text_tokens = self.tokenizer.tokenize(text)\n\n        self.tokens.extend(text_tokens)\n        self.labels.extend(\n            [f\"B-{tags[index]['label']}\"] + [f\"I-{tags[index]['label']}\"] * (len(text_tokens) - 1)\n        )\n\n    @staticmethod\n    def build_label2id(tokens: List[List[str]]):\n        label2id = {}\n        id_counter = 0\n        for token in [token for sublist in tokens for token in sublist]:\n            if token not in label2id:\n                label2id[token] = id_counter\n                id_counter += 1\n        return label2id\n        \n    @staticmethod\n    def find_continuous_ranges(data: List[int]):\n        if not data:\n            return []\n        ranges = []\n        start = prev = data[0]\n        for number in data[1:]:\n            if number != prev + 1:\n                ranges.append(list(range(start, prev + 1)))\n                start = number\n            prev = number\n        ranges.append(list(range(start, prev + 1)))\n        return ranges\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:31:36.617139Z","iopub.execute_input":"2025-02-11T09:31:36.617527Z","iopub.status.idle":"2025-02-11T09:31:36.637873Z","shell.execute_reply.started":"2025-02-11T09:31:36.617503Z","shell.execute_reply":"2025-02-11T09:31:36.636730Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# Preprocessing\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n\ndataset_folder = \"/kaggle/input/maccrobat2018\"\n\nMaccrobat_builder = Preprocessing_Maccrobat(dataset_folder, tokenizer)\ninput_texts, input_labels = Maccrobat_builder.process()\n\nlabel2id = Preprocessing_Maccrobat.build_label2id(input_labels)\nid2label = {v: k for k, v in label2id.items()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:31:57.066896Z","iopub.execute_input":"2025-02-11T09:31:57.067258Z","iopub.status.idle":"2025-02-11T09:32:06.276760Z","shell.execute_reply.started":"2025-02-11T09:31:57.067229Z","shell.execute_reply":"2025-02-11T09:32:06.275971Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# Split\nfrom sklearn.model_selection import train_test_split\n\ninputs_train, inputs_val, labels_train, labels_val = train_test_split(\n    input_texts,\n    input_labels,\n    test_size=0.2,\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:32:06.277893Z","iopub.execute_input":"2025-02-11T09:32:06.278174Z","iopub.status.idle":"2025-02-11T09:32:06.284435Z","shell.execute_reply.started":"2025-02-11T09:32:06.278143Z","shell.execute_reply":"2025-02-11T09:32:06.283498Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nMAX_LEN = 512\n\nclass NER_Dataset(Dataset):\n    def __init__(self, input_texts, input_labels, tokenizer, label2id, max_len=MAX_LEN):\n        super().__init__()\n        self.tokens = input_texts\n        self.labels = input_labels\n        self.tokenizer = tokenizer\n        self.label2id = label2id\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        input_token = self.tokens[idx]\n        label_token = [self.label2id[label] for label in self.labels[idx]]\n\n        input_token = self.tokenizer.convert_tokens_to_ids(input_token)\n        attention_mask = [1] * len(input_token)\n\n        input_ids = self.pad_and_truncate(input_token, pad_id=self.tokenizer.pad_token_id)\n        labels = self.pad_and_truncate(label_token, pad_id=0)\n        attention_mask = self.pad_and_truncate(attention_mask, pad_id=0)\n\n        return {\n            \"input_ids\": torch.as_tensor(input_ids),\n            \"labels\": torch.as_tensor(labels),\n            \"attention_mask\": torch.as_tensor(attention_mask)\n        }\n\n    def pad_and_truncate(self, inputs: List[int], pad_id: int):\n        if len(inputs) < self.max_len:\n            padded_inputs = inputs + [pad_id] * (self.max_len - len(inputs))\n        else:\n            padded_inputs = inputs[:self.max_len]\n        return padded_inputs\n\n    def label2id(self, labels: List[str]):\n        return [self.label2id[label] for label in labels]\n\ntrain_set = NER_Dataset(inputs_train, labels_train, tokenizer, label2id)\nval_set = NER_Dataset(inputs_val, labels_val, tokenizer, label2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:33:59.856121Z","iopub.execute_input":"2025-02-11T09:33:59.856437Z","iopub.status.idle":"2025-02-11T09:33:59.864847Z","shell.execute_reply.started":"2025-02-11T09:33:59.856405Z","shell.execute_reply":"2025-02-11T09:33:59.863875Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"d4data/biomedical-ner-all\",\n    label2id=label2id,\n    id2label=id2label,\n    ignore_mismatched_sizes=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:34:03.457945Z","iopub.execute_input":"2025-02-11T09:34:03.458305Z","iopub.status.idle":"2025-02-11T09:34:04.152448Z","shell.execute_reply.started":"2025-02-11T09:34:03.458276Z","shell.execute_reply":"2025-02-11T09:34:04.151684Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at d4data/biomedical-ner-all and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([84]) in the checkpoint and torch.Size([83]) in the model instantiated\n- classifier.weight: found shape torch.Size([84, 768]) in the checkpoint and torch.Size([83, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    mask = labels != 0\n    predictions = np.argmax(predictions, axis=-1)\n    return accuracy.compute(predictions=predictions[mask], references=labels[mask])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:34:04.153709Z","iopub.execute_input":"2025-02-11T09:34:04.154004Z","iopub.status.idle":"2025-02-11T09:34:04.473682Z","shell.execute_reply.started":"2025-02-11T09:34:04.153981Z","shell.execute_reply":"2025-02-11T09:34:04.472629Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"out_dir\",\n    learning_rate=1e-4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=20,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    optim=\"adamw_torch\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_set,\n    eval_dataset=val_set,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:34:04.475293Z","iopub.execute_input":"2025-02-11T09:34:04.475540Z","iopub.status.idle":"2025-02-11T09:36:29.266317Z","shell.execute_reply.started":"2025-02-11T09:34:04.475520Z","shell.execute_reply":"2025-02-11T09:36:29.265198Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n<ipython-input-64-c0a8e7295acb>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 02:23, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.493638</td>\n      <td>0.109620</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.930282</td>\n      <td>0.248239</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.481573</td>\n      <td>0.445770</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>1.165360</td>\n      <td>0.551993</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.969479</td>\n      <td>0.631121</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.852793</td>\n      <td>0.684067</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.778048</td>\n      <td>0.721269</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.737211</td>\n      <td>0.736184</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.717010</td>\n      <td>0.733201</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>0.693431</td>\n      <td>0.749275</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>No log</td>\n      <td>0.682443</td>\n      <td>0.756566</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>No log</td>\n      <td>0.675045</td>\n      <td>0.750435</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>No log</td>\n      <td>0.678494</td>\n      <td>0.755572</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>No log</td>\n      <td>0.671403</td>\n      <td>0.756152</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>No log</td>\n      <td>0.672816</td>\n      <td>0.758969</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>No log</td>\n      <td>0.663561</td>\n      <td>0.760046</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>No log</td>\n      <td>0.668270</td>\n      <td>0.762946</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>No log</td>\n      <td>0.665903</td>\n      <td>0.763858</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>No log</td>\n      <td>0.664364</td>\n      <td>0.763941</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>No log</td>\n      <td>0.664871</td>\n      <td>0.763692</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=100, training_loss=0.7360022735595703, metrics={'train_runtime': 144.0277, 'train_samples_per_second': 22.218, 'train_steps_per_second': 0.694, 'total_flos': 418702245888000.0, 'train_loss': 0.7360022735595703, 'epoch': 20.0})"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"test_sentence = \"\"\"A 48 year -old female presented with vaginal bleeding and abnormal\nPap smears. Upon diagnosis of invasive non -keratinizing SCC of the cervix , she\nunderwent a radical hysterectomy with salpingo -oophorectomy which demonstrated\npositive spread to the pelvic lymph nodes and the parametrium. Pathological\nexamination revealed that the tumour also extensively involved the lower uterine\nsegment. \"\"\"\n\n# tokenization\n_input = torch.as_tensor([tokenizer.convert_tokens_to_ids(test_sentence.split())])\n\n_input = _input.to(\"cuda\")\n\n# prediction\noutputs = model(_input)\n_, preds = torch.max(outputs.logits, -1)\npreds = preds[0].cpu().numpy()\n\n# decode\nfor token, pred in zip(test_sentence.split(), preds):\n    print(f\"{token}\\t{id2label[pred]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:37:14.723425Z","iopub.execute_input":"2025-02-11T09:37:14.723770Z","iopub.status.idle":"2025-02-11T09:37:14.751028Z","shell.execute_reply.started":"2025-02-11T09:37:14.723746Z","shell.execute_reply":"2025-02-11T09:37:14.750046Z"}},"outputs":[{"name":"stdout","text":"A\tO\n48\tB-Date\nyear\tI-Date\n-old\tI-Age\nfemale\tB-Sex\npresented\tO\nwith\tO\nvaginal\tB-Detailed_description\nbleeding\tB-Sign_symptom\nand\tO\nabnormal\tB-Lab_value\nPap\tB-Lab_value\nsmears.\tB-Lab_value\nUpon\tO\ndiagnosis\tO\nof\tO\ninvasive\tB-Detailed_description\nnon\tO\n-keratinizing\tO\nSCC\tO\nof\tO\nthe\tO\ncervix\tO\n,\tO\nshe\tO\nunderwent\tO\na\tO\nradical\tO\nhysterectomy\tB-Lab_value\nwith\tO\nsalpingo\tB-Lab_value\n-oophorectomy\tI-Detailed_description\nwhich\tO\ndemonstrated\tO\npositive\tB-Lab_value\nspread\tO\nto\tO\nthe\tO\npelvic\tB-Biological_structure\nlymph\tI-Biological_structure\nnodes\tO\nand\tO\nthe\tO\nparametrium.\tB-Diagnostic_procedure\nPathological\tI-Diagnostic_procedure\nexamination\tB-Diagnostic_procedure\nrevealed\tO\nthat\tO\nthe\tO\ntumour\tO\nalso\tO\nextensively\tO\ninvolved\tO\nthe\tO\nlower\tO\nuterine\tO\nsegment.\tI-Detailed_description\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}